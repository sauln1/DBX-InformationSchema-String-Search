{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c09be90-e029-47b1-bdd3-02e9b2735628",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# InformationSchema String Search\n",
    "##### This script is designed to use the information schema in Databricks catalog and search all columns in all available tables for a string. In order to run a search:\n",
    "1. Enter the desired schema and, if you only want to search all columns in a single table, the table name. These are used in a 'LIKE' search, so approximate names are allowed. You can leave both of these blank to search every schema and every table.\n",
    "2. Enter the string you want to search for. You can add in %% surrounding the string within the quotes to use a 'LIKE' search if you don't know the exact string.\n",
    "\n",
    "Any matching table columns will be output after the script is run.\n",
    "\n",
    "#### WARNING: Depending on how large the catalog and target schema is, this may take a lot of time and compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd6a30d1-ef56-4937-90b8-a7a106d9f8b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "targetSchema = \"\"\n",
    "targetTable = \"\"\n",
    "targetString = \"\"\n",
    "\n",
    "dfAllTables = spark.sql(f\"select distinct table_name from information_schema.columns a where a.table_schema like '%{targetSchema}%' and a.table_name like '%{targetTable}%' and a.table_schema != 'information_schema';\")\n",
    "for table in dfAllTables.collect():\n",
    "    tableName = table.table_name\n",
    "    dfAllColumns = spark.sql(f\"select distinct column_name from information_schema.columns a where a.table_name = '{tableName}';\")\n",
    "    for column in dfAllColumns.collect():\n",
    "        columnName = column.column_name\n",
    "        dfHit = spark.sql(f\"select a.{columnName} from {targetSchema}.{table.table_name} a where lower(a.{columnName}) like lower('{targetString}') limit 1;\")\n",
    "\n",
    "        if dfHit.collect():\n",
    "            display(f\"String found in {targetSchema}.{tableName}.{columnName}.\")\n",
    "            display(dfHit.collect())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "InformationSchema String Search",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
